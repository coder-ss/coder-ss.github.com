<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习,概率统计,概率论," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.png?v=5.0.2" />






<meta name="description" content="记录了概率论与数理统计中的一些知识。首先是基本概念，包括概率、概率密度函数、条件概率、贝叶斯公式。然后记录了常见的离散型随机变量分布和连续性随机变量分布。之后介绍了重要的统计量，包括期望、方差、协方差等。然后介绍了Jesen不等式、切比雪夫不等式、大数定理等比较重要的结论。最后介绍了矩估计和极大似然估计。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习之概率统计基础">
<meta property="og:url" content="http://coder-ss.github.io/2016/05/08/machine_learning/probability_statistics/index.html">
<meta property="og:site_name" content="coder_ss's blog">
<meta property="og:description" content="记录了概率论与数理统计中的一些知识。首先是基本概念，包括概率、概率密度函数、条件概率、贝叶斯公式。然后记录了常见的离散型随机变量分布和连续性随机变量分布。之后介绍了重要的统计量，包括期望、方差、协方差等。然后介绍了Jesen不等式、切比雪夫不等式、大数定理等比较重要的结论。最后介绍了矩估计和极大似然估计。">
<meta property="og:image" content="http://7qn7rt.com1.z0.glb.clouddn.com/ml/chebyshev.png">
<meta property="og:updated_time" content="2017-02-24T13:53:49.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习之概率统计基础">
<meta name="twitter:description" content="记录了概率论与数理统计中的一些知识。首先是基本概念，包括概率、概率密度函数、条件概率、贝叶斯公式。然后记录了常见的离散型随机变量分布和连续性随机变量分布。之后介绍了重要的统计量，包括期望、方差、协方差等。然后介绍了Jesen不等式、切比雪夫不等式、大数定理等比较重要的结论。最后介绍了矩估计和极大似然估计。">
<meta name="twitter:image" content="http://7qn7rt.com1.z0.glb.clouddn.com/ml/chebyshev.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://coder-ss.github.io/2016/05/08/machine_learning/probability_statistics/"/>


  <title> 机器学习之概率统计基础 | coder_ss's blog </title>
</head>

<body itemscope itemtype="//schema.org/WebPage" lang="zh-Hans">

  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?7d7157a5836d43e3ad7dbd42e82697f3";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="//schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">coder_ss's blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">do something interesting</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                机器学习之概率统计基础
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-08T22:03:00+08:00" content="2016-05-08">
              2016-05-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine_learning</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/08/machine_learning/probability_statistics/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/08/machine_learning/probability_statistics/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>概率论与数理统计的关注点不同</strong>：</p>
<ul>
<li>概率论：已知总体的分布情况，求某个事件发生的概率。<br><em>例如装箱问题：将12件正品和3件次品随机的装在3个箱子中，每箱装5件，则每箱中恰有1件次品的概率是多少？</em></li>
<li>数理统计：已知样本，估计总体。是概率论的逆向工程<br><em>例如正态分布的矩估计：在正态分布总体中采样得到n个样本</em> $X_1,X_2,\cdots,X_n$，<em>估计该总体的均值和方差。</em></li>
</ul>
<p><strong>概率统计与机器学习</strong>：</p>
<ul>
<li>训练：从样本估计总体（模型），就是数理统计</li>
<li>预测：已知总体（模型），求某个事件发生的概率，就是概率论</li>
</ul>
<p>对于n个特征 $X_1,X_2,\cdots,X_n$ 的样本，每个特征 $X_i$ 的值可以看作是一个分布，对于有监督学习来说每个样本的标签 $Y$ 的值也是一个分布。可以基于各个分布的特性来评估模型和样本。</p>
<h2 id="概率基础"><a href="#概率基础" class="headerlink" title="概率基础"></a>概率基础</h2><p><strong>概率</strong>：</p>
<script type="math/tex; mode=display">P(X)\in[0,1]</script><p><strong>累积分布函数</strong>：</p>
<script type="math/tex; mode=display">\Phi(x)=P(X\leq x)</script><ul>
<li>$\Phi(x)$ 一定为<strong>单增函数</strong></li>
<li>$\min(\Phi(x))=0$，$\max(\Phi(x))=1$</li>
<li>将值域为 $[0,1]$ 的某函数 $yf(x)$ <strong>看成 $x$ 事件的累积概率</strong></li>
<li>若 $y=f(x)$ 可导，则 $p(x)=f\prime(x)$ 为某概率密度函数</li>
</ul>
<p><strong>概率密度函数</strong>，连续随机变量X的分布函数为 </p>
<script type="math/tex; mode=display">\Phi(x)=\int_{-\infty}^{x}f(t)dt</script><p>$f(x)$ 称为 $X$ 的概率密度函数</p>
<p><strong>条件概率</strong>：</p>
<script type="math/tex; mode=display">P(A\mid B)=\frac{P(AB)}{P(B)}</script><p><strong>全概率公式</strong>：</p>
<script type="math/tex; mode=display">P(A)=\sum_{i}P(A\mid B_i)P(B_i)</script><p><strong>贝叶斯(Bayes)公式</strong>：</p>
<script type="math/tex; mode=display">P(B_i\mid A)=\frac{P(AB_i)}{P(A)}=\frac{P(A\mid B_i)P(B_i)}{\sum_{j}P(A\mid B_j)P(B_j)}</script><blockquote>
<p><strong>先验概率、后验概率</strong><br><strong>例：</strong>对以往数据分析表明，当机器调整的好时，产品的合格率为98%，而当机器发生某种故障时，其合格率为55%。每天早上机器开动时，机器调整良好的概率为95%，试求已知某日早上第一件产品是合格品时，机器调整良好的概率是多少？<br><strong>解：</strong>设A为事件“产品合格”，B为事件“机器调整良好”<br>则 $P(A\mid B)=0.98$，$P(A\mid \overline{B})=0.55$<br>     $P(B)=0.95$，$P(\overline{B})=0.05$<br>由贝叶斯公式：</p>
<script type="math/tex; mode=display">\begin{align*} P(B\mid A)&=\frac{P(AB)}{P(A)} \\\\ &=\frac{P(A\mid B)P(B)}{P(A\mid B)P(B)+P(A\mid \overline{B})P(\overline{B})} \\\\ &=\frac{0.98\times 0.95}{0.98\times 0.95+0.55\times 0.05} \\\\ &=0.97 \end{align*}</script><p>概率0.95是由以往数据分析得到的，叫作<strong>先验概率</strong>；<br>得到信息后（第一件产品是合格品）再重新加以修正的概率（0.97）叫<strong>后验概率</strong>。</p>
</blockquote>
<p><strong>两个学派</strong><br>给定某系统的若干样本，求系统的参数</p>
<ul>
<li>频率学派：假定参数是某个/某些未知的定值，求这些参数如何取值，能够使得某目标函数取极大、极小。如矩估计/MLE/MaxEnt/EM等。</li>
<li>贝叶斯学派：假定参数本身是变化的，服从某个分布，求在这个分布约束下使得某目标函数极大/极小。</li>
</ul>
<h2 id="常见分布"><a href="#常见分布" class="headerlink" title="常见分布"></a>常见分布</h2><h3 id="离散型随机变量的分布"><a href="#离散型随机变量的分布" class="headerlink" title="离散型随机变量的分布"></a>离散型随机变量的分布</h3><h4 id="0-1分布"><a href="#0-1分布" class="headerlink" title="0-1分布"></a>0-1分布</h4><script type="math/tex; mode=display">P\{X=k\}=p^k(1-p)^{1-k},\ k=0,1\quad (0<p<1)</script><p><strong>期望</strong>：</p>
<script type="math/tex; mode=display">E(X)=1\cdot p+0\cdot (1-p)=p</script><p><strong>方差</strong>：</p>
<script type="math/tex; mode=display">D(X)=E(X^2)-[E(X)]^2=1^2\cdot p+0^2\cdot (1-p)-p^2=p(1-p)</script><blockquote>
<p>方差推导：</p>
<script type="math/tex; mode=display">\begin{align*}D(X)&= E\{[X-E(X)]^2\} \\\\ &=E\{X^2-2XE(X)+[E(X)]^2\} \\\\ &=E(X^2)-2E(X)E(X)+[E(X)]^2 \\\\ &=E(X^2)-[E(X)]^2 \end{align*}</script></blockquote>
<h4 id="二项-Bernoulli-分布"><a href="#二项-Bernoulli-分布" class="headerlink" title="二项(Bernoulli)分布"></a>二项(Bernoulli)分布</h4><p>试验 $E$ 只有两个可能的结果：$A$ 及 $\overline{A}$，则称 $E$ 为伯努利（Bernoulli）试验。设 $P(A)=p\ (0&lt;p&lt;1)$，此时 $P(\overline{A})=1-p$。将 $E$ 独立重复地进行n次，称为<strong>n重伯努利试验</strong>。n次中 $A$ 出现k次的概率：</p>
<script type="math/tex; mode=display">P\{X=k\}=C_n^kp^k(1-p)^{n-k},\ k=0,1,2,\cdots,n</script><p>记 $X\sim b(n,p)$</p>
<p>设 $X_i$为第 $i$ 次试验，显然 $X_i$服从参数为 $p$ 的0-1分布，且有 $X=\sum_{i=1}^{n} X_i$。</p>
<p><strong>期望</strong>：</p>
<script type="math/tex; mode=display">E(X)=\sum_{i=1}^{n}E(X_i)=np</script><p><strong>方差</strong>：</p>
<script type="math/tex; mode=display">D(X)=\sum_{i=1}^{n}D(X_i)=np(1-p)</script><h4 id="泊松-Possion-分布"><a href="#泊松-Possion-分布" class="headerlink" title="泊松(Possion)分布"></a>泊松(Possion)分布</h4><script type="math/tex; mode=display">P\{X=k\}=\frac{\lambda^ke^{-\lambda}}{k!},\ k=0,1,2,\cdots,</script><p>记 $X\sim\pi(\lambda)$</p>
<p><strong>期望</strong>：</p>
<script type="math/tex; mode=display">E(X)=\sum_{k=0}^{\infty}k\frac{\lambda^ke^{-\lambda}}{k!}=\lambda e^{-\lambda}\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!}=\lambda e^{-\lambda}\cdot e^\lambda=\lambda</script><p><strong>方差</strong>：</p>
<script type="math/tex; mode=display">D(X)=E(X^2)-[E(X)]^2=\lambda</script><p>其中</p>
<script type="math/tex; mode=display">\begin{align*}E(X^2)&=E[X(X-1)+X] \\ &=E[X(X-1)]+E(X)\\&=\sum_{k=0}^{\infty}k(k-1)\frac{\lambda^ke^{-\lambda}}{k!}+\lambda \\ &=\lambda^2e^{-\lambda}\sum_{k=2}^{\infty}\frac{\lambda^{k-2}}{(k-2)!}+\lambda \\ &=\lambda^2e^{-\lambda}e^\lambda+\lambda\\ &=\lambda^2+\lambda \end{align*}</script><blockquote>
<p>泰勒展开与泊松分布<br>$e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\cdots+\frac{x^k}{k!}+R_k$<br>$\xrightarrow{两边同时除以e^x}1=1\cdot e^{-x}+x\cdot e^{-x}+\frac{x^2}{2!}\cdot e^{-x}+ \frac{x^3}{3!}+\cdots + \frac{x^k}{k!}\cdot e^{-x}+R_k\cdot e^{-x}$<br>通项公式 $\frac{x^k}{k!}\cdot e^{-x}\rightarrow \frac{\lambda^k}{k!}\cdot e^{-\lambda}$<br>可以发现，每一项的总和为1（分布函数的最大值）</p>
</blockquote>
<h3 id="连续型随机变量的分布"><a href="#连续型随机变量的分布" class="headerlink" title="连续型随机变量的分布"></a>连续型随机变量的分布</h3><h4 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h4><p>概率密度为：</p>
<script type="math/tex; mode=display">f(x)=\begin{cases} \frac{1}{b-a},\ a<x<b \\ 0,\quad 其他\end{cases}</script><p><strong>期望</strong>：</p>
<script type="math/tex; mode=display">E(X)=\int_{-\infty}^{\infty}xf(x)dx=\int_{a}^{b}\frac{1}{b-a}xdx=\frac{1}{2}(a+b)</script><p><strong>方差</strong>：</p>
<script type="math/tex; mode=display">D(X)=E(X^2)-[E(X)]^2=\int_{a}^{b}\frac{1}{b-a}x^2dx-(\frac{a+b}{2})^2=\frac{(b-a)^2}{12}</script><h4 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h4><p>概率密度函数：</p>
<script type="math/tex; mode=display">f(x)=\begin{cases} \frac{1}{\theta}e^{-x/\theta},\ x>0 \\ 0,\quad 其他\end{cases}</script><p><strong>期望</strong>：</p>
<script type="math/tex; mode=display">E(X)=\int_{-\infty}^{\infty}xf(x)dx=\int_{0}^{\infty}x\cdot\frac{1}{\theta}e^{-x/\theta}dx=-xe^{-x/\theta}\mid_{0}^{\infty} + \int_{0}^{\infty}e^{-x/\theta}dx=\theta</script><p><strong>方差</strong>：</p>
<script type="math/tex; mode=display">D(X)=E(X^2)-[E(X)]^2=\int_{0}^{\infty}x^2\cdot\frac{1}{\theta}e^{-x/\theta}dx-\theta^2=2\theta^2-\theta^2=\theta^2</script><blockquote>
<p>指数分布的无记忆性：$P(x&gt;s+t\mid x&gt;s)=P(x&gt;t)$<br>如果 $x$ 是某电器元器件的寿命，已知元器件使用了 $s$ 小时，则共使用至少 $s+t$ 小时的条件概率，与从未使用开始至少使用 $t$ 小时的概率相等。</p>
</blockquote>
<h4 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a>正态分布</h4><p>连续型随机变量$X$ 概率密度函数：</p>
<script type="math/tex; mode=display">f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},\ \sigma>0,\ -\infty<x<+\infty</script><p>标准正态分布：</p>
<script type="math/tex; mode=display">\varphi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}}</script><p><strong>标准正太分布的期望</strong>：</p>
<script type="math/tex; mode=display">\begin{align*}E(Z)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}te^{-\frac{t^2}{2}}dt=\frac{-1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}} \bigg|_{-\infty}^{\infty}\end{align*} = 0</script><p><strong>标准正太分布的方差</strong>：</p>
<script type="math/tex; mode=display">D(Z)=E(Z^2)-0^2=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}t^2e^{-\frac{t^2}{2}}dt = \frac{-1}{\sqrt{2\pi}}te^{-\frac{t^2}{2}}\bigg|_{-\infty}^{\infty} + \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{t$2}{2}}dt=1</script><p><strong>正态分布的期望</strong>：</p>
<script type="math/tex; mode=display">E(X)=E(\mu+\sigma Z)=\mu</script><p><strong>正态分布的方差</strong>：</p>
<script type="math/tex; mode=display">D(X)=D(\mu+\sigma Z)=D(\sigma Z)=\sigma^2D(Z)=\sigma^2</script><h2 id="重要统计量"><a href="#重要统计量" class="headerlink" title="重要统计量"></a>重要统计量</h2><h3 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h3><ul>
<li>离散型 $\begin{align*}E(X)=\sum_{i}x_ip_i\end{align*}$</li>
<li>连续型 $E(X)=\int _{-\infty}^{\infty}xf(x)dx$</li>
</ul>
<p>期望的性质：</p>
<ul>
<li>$E(kX)=kE(X)$</li>
<li>$E(X+Y)=E(X)+E(Y)$</li>
<li>若 $X$ 和 $Y$ 相互独立，$E(XY)=E(X)E(Y)$。反之不成立，若$E(XY)=E(X)E(Y)$，只能说明 $X$ 和 $Y$ 不相关</li>
</ul>
<blockquote>
<p>独立：$P(AB)=P(A)P(B)$<br>互斥：$P(AB)=0,P(A+B)=P(A)+P(B)$</p>
</blockquote>
<h3 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h3><script type="math/tex; mode=display">D(X)=E\{[X-E(X)]^2\}=E(X^2)-[E(X)]^2</script><p>方差的性质：</p>
<ul>
<li>$D(c)=0$</li>
<li>$D(X+c)=D(X)$</li>
<li>$D(kX)=k^2D(X)$</li>
<li>若 $X$ 和 $Y$ 独立，$D(X+Y)=D(X)+D(Y)$</li>
</ul>
<h3 id="协方差（机器学习中很重要）"><a href="#协方差（机器学习中很重要）" class="headerlink" title="协方差（机器学习中很重要）"></a>协方差（机器学习中很重要）</h3><p>评价两个随机变量的关系</p>
<script type="math/tex; mode=display">Cov(X,Y)=E\{[X-E(X)][Y-E(Y)]\}</script><p>协方差的性质</p>
<ul>
<li>$Cov(X,Y)=Cov(Y,X)$</li>
<li>$Cov(aX+b,cY+d)=acCov(X,Y)$</li>
<li>$Cov(X_1+X_2,Y)=Cov(X_1,Y)+Cov(X_2,Y)$</li>
<li>$Cov(X,Y)=E(XY)-E(X)E(Y)$</li>
<li>$X$ 和 $Y$独立，$Cov(X,Y)=0$</li>
<li>若$Cov(X,Y)=0$，称 $X$ 和 $Y$ 不相关</li>
</ul>
<p>协方差的意义：是两个随机变量具有相同方向变化趋势的度量</p>
<ul>
<li>若 $Cov(X,Y)&gt;0$，它们的变化趋势相同</li>
<li>若 $Cov(X,Y)&lt;0$，它们的变化趋势相反</li>
<li>若 $Cov(X,Y)=0$，称 $X$ 和 $Y$ 不相关</li>
</ul>
<p>协方差可以用于降维（找出与结果不（线性）相关的特征）</p>
<h3 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a>相关系数</h3><script type="math/tex; mode=display">\rho_{XY}=\frac{Cov(X,Y)}{\sqrt{D(X)D(Y)}}</script><blockquote>
<p>注意协方差的上界：$\mid Cov(X,Y)\mid \leq\sqrt{D(X)D(Y)}=\sigma_1\sigma_2$，即$\mid \rho\mid\leq1$，当且仅当X与Y有线性关系时，等号成立。</p>
</blockquote>
<p>可以看出，相关系数时标准尺度下的协方差。</p>
<h3 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h3><p>对于n个随机向量 $(X_1,X_2,\cdots,X_n)$，任意两个元素 $X_i$ 和 $X_j$ 都可以得到一个协方差，从而形成 $n\times n$ 的矩阵，它是一个对称阵。</p>
<script type="math/tex; mode=display">C=\left[\begin{matrix}c_{11}  & c_{12} & \cdots & c_{1n} \\ c_{21} & c_{22} & \cdots & c_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ c_{n1} & c_{n2} & \cdots & c_{nn}\end{matrix}\right]</script><p>其中，$c_{ij}=E{[X_i-E(X_i)][X_j-E(X_j)]}=Cov(X_i,X_j)$</p>
<h3 id="相关系数矩阵"><a href="#相关系数矩阵" class="headerlink" title="相关系数矩阵"></a>相关系数矩阵</h3><p>待补充</p>
<h3 id="矩"><a href="#矩" class="headerlink" title="矩"></a>矩</h3><p>对随机变量X，X的k阶原点矩为</p>
<script type="math/tex; mode=display">E(X^k)</script><p>X的k阶中心矩为</p>
<script type="math/tex; mode=display">E\{[X-E(X)]^k\}</script><p>X和Y的k+l阶混合矩为</p>
<script type="math/tex; mode=display">E(X^kY^l)</script><p>X和Y的k+l阶混合中心矩为</p>
<script type="math/tex; mode=display">E\{[X-E(X)]^k[Y-E(Y)]^l\}</script><p>特殊的矩：</p>
<ul>
<li>期望E(X)是X的一阶原点矩</li>
<li>方差D(X)是X的二阶中心矩</li>
<li>协方差Cov(X,Y)是X和Y的二阶混合中心矩</li>
</ul>
<h2 id="重要定理和不等式"><a href="#重要定理和不等式" class="headerlink" title="重要定理和不等式"></a>重要定理和不等式</h2><h3 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h3><script type="math/tex; mode=display">f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y)</script><p>若$\theta_1,\cdots,\theta_k\geq 0,\ \theta_1+\cdots+\theta_k=1$，则</p>
<script type="math/tex; mode=display">f(\theta_1 x_1+\cdots+\theta_k x_k) \leq \theta_1f(x_1)+\cdots+\theta_k f(x_k)</script><p>即 $f(Ex)=Ef(x)$</p>
<h3 id="切比雪夫不等式"><a href="#切比雪夫不等式" class="headerlink" title="切比雪夫不等式"></a>切比雪夫不等式</h3><p>设随机变量X具有数学期望 $E(X)=\mu$，方差 $D(X)=\sigma^2$，则对于任意正数 $\varepsilon$，不等式 </p>
<script type="math/tex; mode=display">P\{\mid X-\mu\mid\geq\varepsilon\}\leq\frac{\sigma^2}{\varepsilon^2}</script><p>成立。</p>
<blockquote>
<script type="math/tex; mode=display">\begin{align*}P\{\mid X-\mu\mid \geq \varepsilon \} &= \int_{\mid x-\mu\mid\geq\varepsilon}f(x)dx \\\\ &\leq \int_{\mid x-\mu\mid\geq\varepsilon}\frac{\mid x-\mu\mid^2}{\varepsilon^2}f(x)dx \\\\ &\leq \frac{1}{\varepsilon^2}\int_{-\infty}^{\infty}(x-\mu)^2f(x)dx \\\\ &= \frac{1}{\varepsilon^2}\sigma^2 \end{align*}</script></blockquote>
<p>切比雪夫不等式说明，X的方差越小，事件 ${\mid X-\mu\mid&lt; \varepsilon}$ 发生的概率越大。即：X取的值基本上集中在期望 $\mu$ 附近。</p>
<p><img src="http://7qn7rt.com1.z0.glb.clouddn.com/ml/chebyshev.png" alt="Alt text"></p>
<h3 id="大数定理"><a href="#大数定理" class="headerlink" title="大数定理"></a>大数定理</h3><p>设 $X_1,X_2,\cdots$是相互独立，服从同一分布的随机变量序列，且具有数学期望 $E(X_k)=\mu\ (k=1,2,\dots)$。作前n个变量的算术平均 $\begin{align*} Y_n= \frac{1}{n} \sum_{k=1}^{n}X_k \end{align*}$，则对于任意 $\varepsilon&gt;0$，有</p>
<script type="math/tex; mode=display">\begin{align*} \lim \limits_{x\to \infty}P\{\mid Y_n-\mu\mid < \varepsilon \}=1 \end{align*}</script><blockquote>
<script type="math/tex; mode=display">\begin{align*}E(Y)=E\left(\frac{1}{n}\sum_{k=1}^{n}X_k\right) =\frac{1}{n} \sum_{k=1}^{n}E(X_k) = \frac{1}{n}\cdot (n\mu) = \mu \end{align*}</script><script type="math/tex; mode=display">\begin{align*}D(Y)=D\left(\frac{1}{n}\sum_{k=1}^{n}X_k\right) =\frac{1}{n^2} \sum_{k=1}^{n}D(X_k) = \frac{1}{n^2}\cdot (n\sigma^2) = \frac{\sigma^2}{n} \end{align*}</script><p>由切比雪夫不等式有，</p>
<script type="math/tex; mode=display">\begin{align*} P\{\mid Y-\mu_Y \mid < \varepsilon \} \geq 1-\frac{\sigma_Y^2}{\varepsilon^2} \end{align*}</script><p>即</p>
<script type="math/tex; mode=display">\begin{align*} P\left\{\bigg{|} \frac{1}{n} \sum_{k=1}^{n}X_k -\mu \bigg{|} < \varepsilon \right\} \geq 1-\frac{\sigma^2/n}{\varepsilon^2} \end{align*}</script><p>且，$\begin{align*} \lim \limits_{n\to \infty}1- \frac{\sigma^2/n}{\varepsilon^2} = 1\end{align*}$，$\begin{align*} P \left \lbrace \bigg{|} \frac{1}{n} \sum_{k=1}^{n}X_k -\mu \bigg{|} &lt; \varepsilon \right\rbrace \end{align*}$始终小于等于1，<br>所以</p>
<script type="math/tex; mode=display">\begin{align*} \lim \limits_{x\to \infty}P\{\mid Y_n-\mu\mid < \varepsilon \}=1 \end{align*}</script></blockquote>
<p>通俗的说就是样本均值收敛于总体均值。</p>
<p>也称弱大数定理、辛钦大数定理。</p>
<h3 id="伯努利大数定理"><a href="#伯努利大数定理" class="headerlink" title="伯努利大数定理"></a>伯努利大数定理</h3><p>设 $n_A$是n次独立重复事件A发生的次数，P是事件A在每次试验中发生的概率，则对于任意正数 $\varepsilon&gt;0$，有 </p>
<script type="math/tex; mode=display">\begin{align*} \lim \limits_{n\to\infty}P\left\{\bigg{|}\frac{n_A}{n}-p\bigg{|}< \varepsilon \right\} = 1\end{align*}</script><h3 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h3><p>设随机变量 $X_1,X_2,\cdots,X_n,\cdots$ 独立同分布，且具有期望和方差：$E(X_k)=\mu,\ D(X_k)=\sigma^2\ (k=1,2,\cdots)$，则随机变量之和 $\begin{align*} \sum_{k=1}^{n}X_k \end{align*}$ 的标准化变量 </p>
<script type="math/tex; mode=display">\begin{align*} Y_n=\frac{\sum_{k=1}^{n}X_k-n\mu}{\sqrt{n}\sigma} \end{align*}</script><p>的分布收敛到标准正态分布。</p>
<p>即n充分大时</p>
<script type="math/tex; mode=display">\begin{align*} \frac{\sum_{k=1}^{n}X_k-n\mu}{\sqrt{n}\sigma} \end{align*} \overset {近似地}{\sim} N(0,1)</script><p>或</p>
<script type="math/tex; mode=display">\begin{align*} \frac{\overline{X}-\mu}{\sigma/\sqrt{n}} \overset {近似地}\sim N(0,1) \end{align*}</script><p>或</p>
<script type="math/tex; mode=display">\begin{align*} \overline{X} \overset {近似地}\sim N(\mu,\frac{\sigma^2}{n}) \end{align*}</script><h2 id="用样本估计总体参数"><a href="#用样本估计总体参数" class="headerlink" title="用样本估计总体参数"></a>用样本估计总体参数</h2><p>总体X的分布函数$F(x;\theta)$的形式已知，$\theta$ 无法求解，但可以利用X的样本 $X_1,X_2,\cdots,X_n$ 得到一个估计值 $\hat{\theta}$。常用的估计方法有矩估计和极大似然估计。</p>
<h3 id="矩估计"><a href="#矩估计" class="headerlink" title="矩估计"></a>矩估计</h3><p>k阶样本的原点矩为 $\begin{align*}A_k=\frac{1}{n} \sum_{i=1}^{n} X_i^k \end{align*}$<br>k阶样本的中心矩为 $\begin{align*} M_k = \frac{1}{n} \sum_{i-1}^{n}  (X_i - \overline{X})^k\end{align*}$</p>
<p>矩估计就是用样本矩作为相应的总体矩的估计量。</p>
<blockquote>
<p><strong>例：</strong>设总体X在[a,b]上服从均匀分布，a,b未知。 $X_1,X_2,\cdots,X_n$ 是来自X的样本，试求a,b的矩估计量。<br><strong>解：</strong></p>
<script type="math/tex; mode=display">\mu_1=E(X)=(a+b)/2</script><script type="math/tex; mode=display">\mu_2=E(X^2)=D(X)+[E(X)]^2=(b-a)^2/12+(a+b)^2/4</script><p>即</p>
<script type="math/tex; mode=display">\left\lbrace \begin{align*} a+b &=  2\mu_1 \\\\ b-a &= \sqrt{12(\mu_2-\mu_1^2)} \end{align*}\right.</script><p>解得</p>
<script type="math/tex; mode=display">a=\mu_1-\sqrt{3(\mu_2-\mu_1^2)},\ b=\mu\_1 + \sqrt{3(\mu_2-\mu_1^2)}</script><p>用样本矩作为总体矩的估计就可以得到a,b的估计量</p>
<script type="math/tex; mode=display">\begin{align*}\hat{a} = A_1-\sqrt{3(A_2-A_1^2)}=\overline{X}-\sqrt{3\left(\frac{1}{n} \sum_{i=1}^{n} X_i^2 - \overline{X}^2\right)} \end{align*}</script><script type="math/tex; mode=display">\begin{align*}\hat{b} = A_1+\sqrt{3(A_2-A_1^2)}=\overline{X}+ \sqrt{3\left(\frac{1}{n} \sum_{i=1}^{n} X_i^2 - \overline{X}^2\right)} \end{align*}</script></blockquote>
<h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>设总体X的分布（概率密度）为$f(x;\theta)$， $X_1,X_2,\cdots,X_n$ 是来自X的样本，其联合密度为</p>
<script type="math/tex; mode=display">L(x_1,x_2,\cdots,x_n;\theta)=\prod_{i=1}^{n}f(x_i;\theta)</script><p>$x_1,x_2,\cdots,x_n$ 是相应这个样本的样本值。</p>
<p>这里，$\theta$ 被看作固定但未知的参数；而样本已存在，$x_1,x_2,\cdots,x_n$ 是固定的，$L(x,\theta)$ 是关于 $\theta$ 的函数，即似然函数。</p>
<p>求参数 $\theta$ 的值，使得似然函数取极大值，这种方法就是极大似然估计。</p>
<blockquote>
<p>不同的分布有不同的 $\theta$ 值，相同分布不同参数也是不同的 $\theta$ 值。这里讨论分布只有一个变量 $\theta$ ，更多情况下会是 $\theta_1,\theta_2,\cdots$ ，也是类似的。</p>
<p><strong>从贝叶斯公式来看极大似然</strong><br>给定样本D，在这些样本中计算某结论 $A_1,A_2,\cdots,A_n$ 出现的概率，即 $P(A_i|D)$<br>极大似然就是找出 $P(A_i|D)$ 最大时的 $A_i$ 的值</p>
<script type="math/tex; mode=display">\begin{align*} \max P(A_i|D) &= \max \frac{P(D|A_i)P(A_i)}{P(D)} \\\\ &=\max (P(D|A_i)P(A_i)) \\\\ &\xrightarrow{P(A_i)近似相等} \max P(D|A_i)\end{align*}</script></blockquote>
<p>实际中，由于求导的需要，往往将似然函数取对数，得到对数似然函数。</p>
<script type="math/tex; mode=display">\log L(x_1,x_2,\cdots,x_n;\theta)=\sum_{i=1}^{n}\log f(x_1,x_2,\cdots,x_n;\theta)</script><p>用导数等于0，求最大值：</p>
<script type="math/tex; mode=display">\frac{d}{d\theta}\ln L(\theta)=0</script><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>七月算法机器学习课程<br>概率论与数理统计 浙江大学<br>机器学习 周志华</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag">#机器学习</a>
          
            <a href="/tags/概率统计/" rel="tag">#概率统计</a>
          
            <a href="/tags/概率论/" rel="tag">#概率论</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/05/04/machine_learning/advanced-mathematics/" rel="next" title="机器学习之高数基础">
                <i class="fa fa-chevron-left"></i> 机器学习之高数基础
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/05/10/machine_learning/matrix/" rel="prev" title="机器学习之矩阵基础">
                机器学习之矩阵基础 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2016/05/08/machine_learning/probability_statistics/"
           data-title="机器学习之概率统计基础" data-url="http://coder-ss.github.io/2016/05/08/machine_learning/probability_statistics/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/favicon.png"
               alt="coder-ss" />
          <p class="site-author-name" itemprop="name">coder-ss</p>
          <p class="site-description motion-element" itemprop="description">do something interesting</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">24</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">26</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#概率基础"><span class="nav-number">2.</span> <span class="nav-text">概率基础</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常见分布"><span class="nav-number">3.</span> <span class="nav-text">常见分布</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#离散型随机变量的分布"><span class="nav-number">3.1.</span> <span class="nav-text">离散型随机变量的分布</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#0-1分布"><span class="nav-number">3.1.1.</span> <span class="nav-text">0-1分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二项-Bernoulli-分布"><span class="nav-number">3.1.2.</span> <span class="nav-text">二项(Bernoulli)分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#泊松-Possion-分布"><span class="nav-number">3.1.3.</span> <span class="nav-text">泊松(Possion)分布</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#连续型随机变量的分布"><span class="nav-number">3.2.</span> <span class="nav-text">连续型随机变量的分布</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#均匀分布"><span class="nav-number">3.2.1.</span> <span class="nav-text">均匀分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#指数分布"><span class="nav-number">3.2.2.</span> <span class="nav-text">指数分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#正态分布"><span class="nav-number">3.2.3.</span> <span class="nav-text">正态分布</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#重要统计量"><span class="nav-number">4.</span> <span class="nav-text">重要统计量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#期望"><span class="nav-number">4.1.</span> <span class="nav-text">期望</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方差"><span class="nav-number">4.2.</span> <span class="nav-text">方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#协方差（机器学习中很重要）"><span class="nav-number">4.3.</span> <span class="nav-text">协方差（机器学习中很重要）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#相关系数"><span class="nav-number">4.4.</span> <span class="nav-text">相关系数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#协方差矩阵"><span class="nav-number">4.5.</span> <span class="nav-text">协方差矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#相关系数矩阵"><span class="nav-number">4.6.</span> <span class="nav-text">相关系数矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#矩"><span class="nav-number">4.7.</span> <span class="nav-text">矩</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#重要定理和不等式"><span class="nav-number">5.</span> <span class="nav-text">重要定理和不等式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Jensen不等式"><span class="nav-number">5.1.</span> <span class="nav-text">Jensen不等式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#切比雪夫不等式"><span class="nav-number">5.2.</span> <span class="nav-text">切比雪夫不等式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#大数定理"><span class="nav-number">5.3.</span> <span class="nav-text">大数定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#伯努利大数定理"><span class="nav-number">5.4.</span> <span class="nav-text">伯努利大数定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#中心极限定理"><span class="nav-number">5.5.</span> <span class="nav-text">中心极限定理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#用样本估计总体参数"><span class="nav-number">6.</span> <span class="nav-text">用样本估计总体参数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#矩估计"><span class="nav-number">6.1.</span> <span class="nav-text">矩估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#极大似然估计"><span class="nav-number">6.2.</span> <span class="nav-text">极大似然估计</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">7.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">coder-ss</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.2"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"totoo"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    <script src="/vendors/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  






  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

  


</body>
</html>
